{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mlc-ai/notebooks/blob/main/2_tensor_program_abstraction.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mpn1ti5Urdsv"
      },
      "source": [
        "# Tensor Program Abstraction in Action\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qXysoqn-vZuF"
      },
      "source": [
        "## Install packages \n",
        "\n",
        "For the purpose of this course, we will use some on-going development in tvm, which is an open source machine learning compilation framework. We provide the following command to install a packaged version for mlc course."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Xe3vClsD9jlq",
        "outputId": "6d336487-f44b-45cc-ca2c-bae60a295cdb"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Looking in indexes: https://pypi.tuna.tsinghua.edu.cn/simple\n",
            "Looking in links: https://mlc.ai/wheels\n",
            "Collecting mlc-ai-nightly\n",
            "  Downloading https://github.com/mlc-ai/utils/releases/download/v0.9.dev0/mlc_ai_nightly-0.9.dev2227%2Bg06c0ef3be-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (45.5 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m45.5/45.5 MB\u001b[0m \u001b[31m171.9 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:08\u001b[0m\n",
            "\u001b[?25hCollecting synr==0.6.0\n",
            "  Downloading https://pypi.tuna.tsinghua.edu.cn/packages/46/5e/0e5e6780784907acf88ce92095831f9b6e99bbfffb0a2356ddf13967df5f/synr-0.6.0-py3-none-any.whl (18 kB)\n",
            "Requirement already satisfied: decorator in /home/zkti/anaconda3/envs/lxyMLC/lib/python3.7/site-packages (from mlc-ai-nightly) (5.1.1)\n",
            "Requirement already satisfied: tornado in /home/zkti/anaconda3/envs/lxyMLC/lib/python3.7/site-packages (from mlc-ai-nightly) (6.2)\n",
            "Collecting numpy\n",
            "  Using cached https://pypi.tuna.tsinghua.edu.cn/packages/6d/ad/ff3b21ebfe79a4d25b4a4f8e5cf9fd44a204adb6b33c09010f566f51027a/numpy-1.21.6-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (15.7 MB)\n",
            "Collecting cloudpickle\n",
            "  Downloading https://pypi.tuna.tsinghua.edu.cn/packages/25/40/2c9db9cfb85a8a21c61528f6660c47662b3e59576efac610d8268d47abba/cloudpickle-2.1.0-py3-none-any.whl (25 kB)\n",
            "Requirement already satisfied: attrs in /home/zkti/anaconda3/envs/lxyMLC/lib/python3.7/site-packages (from mlc-ai-nightly) (22.1.0)\n",
            "Collecting scipy\n",
            "  Using cached https://pypi.tuna.tsinghua.edu.cn/packages/58/4f/11f34cfc57ead25752a7992b069c36f5d18421958ebd6466ecd849aeaf86/scipy-1.7.3-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (38.1 MB)\n",
            "Requirement already satisfied: psutil in /home/zkti/anaconda3/envs/lxyMLC/lib/python3.7/site-packages (from mlc-ai-nightly) (5.9.2)\n",
            "Installing collected packages: synr, numpy, cloudpickle, scipy, mlc-ai-nightly\n",
            "Successfully installed cloudpickle-2.1.0 mlc-ai-nightly-0.9.dev2227+g06c0ef3be numpy-1.21.6 scipy-1.7.3 synr-0.6.0\n"
          ]
        }
      ],
      "source": [
        "!python3 -m  pip install mlc-ai-nightly -f https://mlc.ai/wheels"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BBIuE2jc1DaU"
      },
      "source": [
        "## Constructing Tensor Program\n",
        "\n",
        "Let us begin by constructing a tensor program that performs addition among two vectors."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "vvfOgcu-YdaB"
      },
      "outputs": [],
      "source": [
        "import tvm\n",
        "from tvm.ir.module import IRModule\n",
        "from tvm.script import tir as T\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "qCViJNUNYfTW"
      },
      "outputs": [],
      "source": [
        "@tvm.script.ir_module\n",
        "class MyModule:\n",
        "    @T.prim_func\n",
        "    def main(A: T.Buffer[128, \"float32\"], \n",
        "             B: T.Buffer[128, \"float32\"], \n",
        "             C: T.Buffer[128, \"float32\"]):\n",
        "        # extra annotations for the function\n",
        "        T.func_attr({\"global_symbol\": \"main\", \"tir.noalias\": True})\n",
        "        for i in range(128):\n",
        "            with T.block(\"C\"):\n",
        "                # declare a data parallel iterator on spatial domain\n",
        "                vi = T.axis.spatial(128, i)\n",
        "                C[vi] = A[vi] + B[vi]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4PJd0Pw8zVQD"
      },
      "source": [
        "TVMScript is a way for us to express tensor program in python ast. Note that this code do not actually correspond to a python program, but a tensor program  that can be used in MLC process. The language is designed to align with python syntax with additional structures to facilitate analysis and transformation. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QKsLAcDB8Npx",
        "outputId": "8534ef46-c656-4f36-961c-f6e59e04ad6d"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tvm.ir.module.IRModule"
            ]
          },
          "execution_count": 5,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "type(MyModule)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GpdPoa5q8Sj7"
      },
      "source": [
        "MyModule is an instance of an **IRModule** data structure, which is used to hold a collection of tensor functions. \n",
        "\n",
        "We can use the script function get a string based representation of the IRModule. This function is quite useful for inspecting the module during each step of transformation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VXy-4v3Czax9",
        "outputId": "c933d1e0-42d5-4df2-ad9a-6eb997deb10c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "@tvm.script.ir_module\n",
            "class Module:\n",
            "    @T.prim_func\n",
            "    def main(A: T.Buffer[128, \"float32\"], B: T.Buffer[128, \"float32\"], C: T.Buffer[128, \"float32\"]) -> None:\n",
            "        # function attr dict\n",
            "        T.func_attr({\"global_symbol\": \"main\", \"tir.noalias\": True})\n",
            "        # body\n",
            "        # with T.block(\"root\")\n",
            "        for i in T.serial(128):\n",
            "            with T.block(\"C\"):\n",
            "                vi = T.axis.spatial(128, i)\n",
            "                T.reads(A[vi], B[vi])\n",
            "                T.writes(C[vi])\n",
            "                C[vi] = A[vi] + B[vi]\n",
            "    \n"
          ]
        }
      ],
      "source": [
        "print(MyModule.script())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tOMSOuW6aIJg"
      },
      "source": [
        "### Build and run\n",
        "\n",
        "Any any time point, we can turn an IRModule to runnable functions by calling a build function."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oESoqN-xaTCf",
        "outputId": "58119fbd-b737-400d-bd94-776af0709501"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "<class 'tvm.driver.build_module.OperatorModule'>\n"
          ]
        }
      ],
      "source": [
        "rt_mod = tvm.build(MyModule, target=\"llvm\")  # The module for CPU backends.\n",
        "print(type(rt_mod))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y2ZfGrH1z6SV"
      },
      "source": [
        "After build, mod contains a collection of runnable functions. We can retrieve each function by its name."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "5I3GqwnRz-Ne"
      },
      "outputs": [],
      "source": [
        "func = rt_mod[\"main\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bngdW1eVl683",
        "outputId": "d5e0437c-bf16-4107-aa41-e11a4e1865ce"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<tvm.runtime.packed_func.PackedFunc at 0x7f356a541b90>"
            ]
          },
          "execution_count": 9,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "func"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "DKxo8uq_mNlp"
      },
      "outputs": [],
      "source": [
        "a = tvm.nd.array(np.arange(128, dtype=\"float32\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "1hAFAqv_mP8W"
      },
      "outputs": [],
      "source": [
        "b = tvm.nd.array(np.ones(128, dtype=\"float32\")) "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "TseB1UBumivT"
      },
      "outputs": [],
      "source": [
        "c = tvm.nd.empty((128,), dtype=\"float32\") "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p68xZ0_P0MPw"
      },
      "source": [
        "To invoke the function, we can create three NDArrays in the tvm runtime, and then invoke the generated function."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "SMkcgO-L0Xr5"
      },
      "outputs": [],
      "source": [
        "func(a, b, c)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AakkTpE50b6o",
        "outputId": "43971a60-2fbb-41bb-cc47-c496bfe5dda2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[  0.   1.   2.   3.   4.   5.   6.   7.   8.   9.  10.  11.  12.  13.\n",
            "  14.  15.  16.  17.  18.  19.  20.  21.  22.  23.  24.  25.  26.  27.\n",
            "  28.  29.  30.  31.  32.  33.  34.  35.  36.  37.  38.  39.  40.  41.\n",
            "  42.  43.  44.  45.  46.  47.  48.  49.  50.  51.  52.  53.  54.  55.\n",
            "  56.  57.  58.  59.  60.  61.  62.  63.  64.  65.  66.  67.  68.  69.\n",
            "  70.  71.  72.  73.  74.  75.  76.  77.  78.  79.  80.  81.  82.  83.\n",
            "  84.  85.  86.  87.  88.  89.  90.  91.  92.  93.  94.  95.  96.  97.\n",
            "  98.  99. 100. 101. 102. 103. 104. 105. 106. 107. 108. 109. 110. 111.\n",
            " 112. 113. 114. 115. 116. 117. 118. 119. 120. 121. 122. 123. 124. 125.\n",
            " 126. 127.]\n",
            "[1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
            " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
            " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
            " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
            " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
            " 1. 1. 1. 1. 1. 1. 1. 1.]\n",
            "[  1.   2.   3.   4.   5.   6.   7.   8.   9.  10.  11.  12.  13.  14.\n",
            "  15.  16.  17.  18.  19.  20.  21.  22.  23.  24.  25.  26.  27.  28.\n",
            "  29.  30.  31.  32.  33.  34.  35.  36.  37.  38.  39.  40.  41.  42.\n",
            "  43.  44.  45.  46.  47.  48.  49.  50.  51.  52.  53.  54.  55.  56.\n",
            "  57.  58.  59.  60.  61.  62.  63.  64.  65.  66.  67.  68.  69.  70.\n",
            "  71.  72.  73.  74.  75.  76.  77.  78.  79.  80.  81.  82.  83.  84.\n",
            "  85.  86.  87.  88.  89.  90.  91.  92.  93.  94.  95.  96.  97.  98.\n",
            "  99. 100. 101. 102. 103. 104. 105. 106. 107. 108. 109. 110. 111. 112.\n",
            " 113. 114. 115. 116. 117. 118. 119. 120. 121. 122. 123. 124. 125. 126.\n",
            " 127. 128.]\n"
          ]
        }
      ],
      "source": [
        "print(a)\n",
        "print(b)\n",
        "print(c)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i_MIDZCOcmwp"
      },
      "source": [
        "## Transform the Tensor Program\n",
        "\n",
        "Now let us start to transform the Tensor Program. A tensor prigram can be transformed using an auxiliary data structure called schedule.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xwyjwh51cjWI",
        "outputId": "d8a8687a-a722-4899-c181-9ca90c7d841e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "<class 'tvm.tir.schedule.schedule.Schedule'>\n"
          ]
        }
      ],
      "source": [
        "sch = tvm.tir.Schedule(MyModule)\n",
        "print(type(sch))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Dw7Fgw8o8HPm"
      },
      "source": [
        "Let us first try to split the loops"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kNQf8D0ic4me",
        "outputId": "1cbfd7f9-a807-4571-b2e3-66ffe052037d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "@tvm.script.ir_module\n",
            "class Module:\n",
            "    @T.prim_func\n",
            "    def main(A: T.Buffer[128, \"float32\"], B: T.Buffer[128, \"float32\"], C: T.Buffer[128, \"float32\"]) -> None:\n",
            "        # function attr dict\n",
            "        T.func_attr({\"global_symbol\": \"main\", \"tir.noalias\": True})\n",
            "        # body\n",
            "        # with T.block(\"root\")\n",
            "        for i_0, i_1, i_2 in T.grid(8, 4, 4):\n",
            "            with T.block(\"C\"):\n",
            "                vi = T.axis.spatial(128, i_0 * 16 + i_1 * 4 + i_2)\n",
            "                T.reads(A[vi], B[vi])\n",
            "                T.writes(C[vi])\n",
            "                C[vi] = A[vi] + B[vi]\n",
            "    \n"
          ]
        }
      ],
      "source": [
        "# Get block by its name\n",
        "block_c = sch.get_block(\"C\")\n",
        "# Get loops surronding the block\n",
        "(i,) = sch.get_loops(block_c)\n",
        "# Tile the loop nesting.\n",
        "i_0, i_1, i_2 = sch.split(i, factors=[None, 4, 4])\n",
        "print(sch.mod.script())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nzrbvqBSdC-D"
      },
      "source": [
        "We can also reorder the loops. Now we move loop i_2 to outside of i_1.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yJWBq7lRdDmn",
        "outputId": "1957cf67-f51c-4af1-c5ca-16c9718181a0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "@tvm.script.ir_module\n",
            "class Module:\n",
            "    @T.prim_func\n",
            "    def main(A: T.Buffer[128, \"float32\"], B: T.Buffer[128, \"float32\"], C: T.Buffer[128, \"float32\"]) -> None:\n",
            "        # function attr dict\n",
            "        T.func_attr({\"global_symbol\": \"main\", \"tir.noalias\": True})\n",
            "        # body\n",
            "        # with T.block(\"root\")\n",
            "        for i_0, i_2, i_1 in T.grid(8, 4, 4):\n",
            "            with T.block(\"C\"):\n",
            "                vi = T.axis.spatial(128, i_0 * 16 + i_1 * 4 + i_2)\n",
            "                T.reads(A[vi], B[vi])\n",
            "                T.writes(C[vi])\n",
            "                C[vi] = A[vi] + B[vi]\n",
            "    \n"
          ]
        }
      ],
      "source": [
        "sch.reorder(i_0, i_2, i_1)\n",
        "print(sch.mod.script())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UmUr6b_L07-8"
      },
      "source": [
        "Finally, we can add hints to the program generator that we want to vectorize the inner most loop."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 84,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u95zQFuldHs_",
        "outputId": "b2205442-ac70-405c-8f42-ddb23e46e012"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "@tvm.script.ir_module\n",
            "class Module:\n",
            "    @tir.prim_func\n",
            "    def main(A_1: tir.Buffer[128, \"float32\"], B_1: tir.Buffer[128, \"float32\"], C_1: tir.Buffer[128, \"float32\"]) -> None:\n",
            "        # function attr dict\n",
            "        tir.func_attr({\"global_symbol\": \"main\", \"tir.noalias\": True})\n",
            "        # body\n",
            "        # with tir.block(\"root\")\n",
            "        for i_0, i_2, i_1 in tir.grid(8, 4, 4):\n",
            "            with tir.block(\"C\"):\n",
            "                vi = tir.axis.spatial(128, i_0 * 16 + i_1 * 4 + i_2)\n",
            "                tir.reads(A_1[vi], B_1[vi])\n",
            "                tir.writes(C_1[vi])\n",
            "                C_1[vi] = A_1[vi] + B_1[vi]\n",
            "    \n"
          ]
        }
      ],
      "source": [
        "print(sch.mod.script())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "m7NFPx9fy5Wy"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "@tvm.script.ir_module\n",
            "class Module:\n",
            "    @T.prim_func\n",
            "    def main(A: T.Buffer[128, \"float32\"], B: T.Buffer[128, \"float32\"], C: T.Buffer[128, \"float32\"]) -> None:\n",
            "        # function attr dict\n",
            "        T.func_attr({\"global_symbol\": \"main\", \"tir.noalias\": True})\n",
            "        # body\n",
            "        # with T.block(\"root\")\n",
            "        for i_0 in T.parallel(8):\n",
            "            for i_2, i_1 in T.grid(4, 4):\n",
            "                with T.block(\"C\"):\n",
            "                    vi = T.axis.spatial(128, i_0 * 16 + i_1 * 4 + i_2)\n",
            "                    T.reads(A[vi], B[vi])\n",
            "                    T.writes(C[vi])\n",
            "                    C[vi] = A[vi] + B[vi]\n",
            "    \n"
          ]
        }
      ],
      "source": [
        "sch.parallel(i_0)\n",
        "print(sch.mod.script())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OhGlqLTG_tNv"
      },
      "source": [
        "We can build and run the transformed program\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "sCIYSDrI_wGq"
      },
      "outputs": [],
      "source": [
        "transformed_mod = tvm.build(sch.mod, target=\"llvm\")  # The module for CPU backends.\n",
        "transformed_mod[\"main\"](a, b, c)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wj_01P4mAfu2"
      },
      "source": [
        "## Constructing Tensor Program using Tensor Expression\n",
        "\n",
        "In the previous example, we directly use TVMScript to construct the tensor program. In practice, it is usually helpful to construct these functions pragmatically from existing definitions. Tensor expression is an API that helps us to build some of the expression-like array computations."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TZAPcqbGAesY",
        "outputId": "ac4d6407-8dea-4c75-d166-e071ffee8783"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "@tvm.script.ir_module\n",
            "class Module:\n",
            "    @T.prim_func\n",
            "    def main(A: T.Buffer[128, \"float32\"], B: T.Buffer[128, \"float32\"], C: T.Buffer[128, \"float32\"]) -> None:\n",
            "        # function attr dict\n",
            "        T.func_attr({\"global_symbol\": \"main\", \"tir.noalias\": True})\n",
            "        # body\n",
            "        # with T.block(\"root\")\n",
            "        for i0 in T.serial(128):\n",
            "            with T.block(\"C\"):\n",
            "                i = T.axis.spatial(128, i0)\n",
            "                T.reads(A[i], B[i])\n",
            "                T.writes(C[i])\n",
            "                C[i] = A[i] + B[i]\n",
            "    \n"
          ]
        }
      ],
      "source": [
        "# namespace for tensor expression utility\n",
        "from tvm import te\n",
        "\n",
        "# declare the computation using the expression API\n",
        "A = te.placeholder((128, ), name=\"A\")\n",
        "B = te.placeholder((128, ), name=\"B\")\n",
        "C = te.compute((128,), lambda i: A[i] + B[i], name=\"C\")\n",
        "\n",
        "# create a function with the specified list of arguments. \n",
        "func = te.create_prim_func([A, B, C])\n",
        "# mark that the function name is main\n",
        "func = func.with_attr(\"global_symbol\", \"main\")\n",
        "ir_mod_from_te = IRModule({\"main\": func})\n",
        "\n",
        "print(ir_mod_from_te.script())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GEqpO14Lf0Lq"
      },
      "source": [
        "## Transforming a matrix multiplication program\n",
        "\n",
        "In the above example, we showed how to transform an vector add. Now let us try to apply that to a slightly more complicated program(matrix multiplication). Let us first try to build the initial code using the tensor expression API.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R9ExHE3BfYYv",
        "outputId": "0d82d527-8051-4bc3-c3a7-0cbb12d9bcce"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "@tvm.script.ir_module\n",
            "class Module:\n",
            "    @T.prim_func\n",
            "    def main(A: T.Buffer[(1024, 1024), \"float32\"], B: T.Buffer[(1024, 1024), \"float32\"], C: T.Buffer[(1024, 1024), \"float32\"]) -> None:\n",
            "        # function attr dict\n",
            "        T.func_attr({\"global_symbol\": \"main\", \"tir.noalias\": True})\n",
            "        # body\n",
            "        # with T.block(\"root\")\n",
            "        for i0, i1, i2 in T.grid(1024, 1024, 1024):\n",
            "            with T.block(\"C\"):\n",
            "                m, n, k = T.axis.remap(\"SSR\", [i0, i1, i2])\n",
            "                T.reads(A[m, k], B[k, n])\n",
            "                T.writes(C[m, n])\n",
            "                with T.init():\n",
            "                    C[m, n] = T.float32(0)\n",
            "                C[m, n] = C[m, n] + A[m, k] * B[k, n]\n",
            "    \n",
            "Baseline: 3.197013\n"
          ]
        }
      ],
      "source": [
        "from tvm import te\n",
        "\n",
        "M = 1024\n",
        "K = 1024\n",
        "N = 1024\n",
        "\n",
        "# The default tensor type in tvm\n",
        "dtype = \"float32\"\n",
        "\n",
        "target = \"llvm\"\n",
        "dev = tvm.device(target, 0)\n",
        "\n",
        "# Algorithm\n",
        "k = te.reduce_axis((0, K), \"k\")\n",
        "A = te.placeholder((M, K), name=\"A\")\n",
        "B = te.placeholder((K, N), name=\"B\")\n",
        "C = te.compute((M, N), lambda m, n: te.sum(A[m, k] * B[k, n], axis=k), name=\"C\")\n",
        "\n",
        "# Default schedule\n",
        "func = te.create_prim_func([A, B, C])\n",
        "func = func.with_attr(\"global_symbol\", \"main\")\n",
        "ir_module = IRModule({\"main\": func})\n",
        "print(ir_module.script())\n",
        "\n",
        "\n",
        "func = tvm.build(ir_module, target=\"llvm\")  # The module for CPU backends.\n",
        "\n",
        "a = tvm.nd.array(np.random.rand(M, K).astype(dtype), dev)\n",
        "b = tvm.nd.array(np.random.rand(K, N).astype(dtype), dev)\n",
        "c = tvm.nd.array(np.zeros((M, N), dtype=dtype), dev)\n",
        "func(a, b, c)\n",
        "\n",
        "evaluator = func.time_evaluator(func.entry_name, dev, number=1)\n",
        "print(\"Baseline: %f\" % evaluator(a, b, c).mean)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "swj-gMz-1vBE"
      },
      "source": [
        "We can transform the loop access pattern to make it more cache friendly. Let us use the following schedule."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W60q68KRgdNL",
        "outputId": "b49a101e-5148-4cf0-df88-0e112e741381"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "<class 'tvm.tir.schedule.schedule.Schedule'>\n",
            "@tvm.script.ir_module\n",
            "class Module:\n",
            "    @T.prim_func\n",
            "    def main(A: T.Buffer[(1024, 1024), \"float32\"], B: T.Buffer[(1024, 1024), \"float32\"], C: T.Buffer[(1024, 1024), \"float32\"]) -> None:\n",
            "        # function attr dict\n",
            "        T.func_attr({\"global_symbol\": \"main\", \"tir.noalias\": True})\n",
            "        # body\n",
            "        # with T.block(\"root\")\n",
            "        for i0_0, i1_0, i2, i0_1, i1_1 in T.grid(32, 32, 1024, 32, 32):\n",
            "            with T.block(\"C\"):\n",
            "                m = T.axis.spatial(1024, i0_0 * 32 + i0_1)\n",
            "                n = T.axis.spatial(1024, i1_0 * 32 + i1_1)\n",
            "                k = T.axis.reduce(1024, i2)\n",
            "                T.reads(A[m, k], B[k, n])\n",
            "                T.writes(C[m, n])\n",
            "                with T.init():\n",
            "                    C[m, n] = T.float32(0)\n",
            "                C[m, n] = C[m, n] + A[m, k] * B[k, n]\n",
            "    \n",
            "after transformation: 0.226121\n"
          ]
        }
      ],
      "source": [
        "sch = tvm.tir.Schedule(ir_module)\n",
        "print(type(sch))\n",
        "block_c = sch.get_block(\"C\")\n",
        "# Get loops surronding the block\n",
        "(y, x, k) = sch.get_loops(block_c)\n",
        "block_size = 32\n",
        "yo, yi = sch.split(y, [None, block_size])\n",
        "xo, xi = sch.split(x, [None, block_size])\n",
        "\n",
        "sch.reorder(yo, xo, k, yi, xi)\n",
        "print(sch.mod.script())\n",
        "\n",
        "func = tvm.build(sch.mod, target=\"llvm\")  # The module for CPU backends.\n",
        "\n",
        "c = tvm.nd.array(np.zeros((M, N), dtype=dtype), dev)\n",
        "func(a, b, c)\n",
        "\n",
        "evaluator = func.time_evaluator(func.entry_name, dev, number=1)\n",
        "print(\"after transformation: %f\" % evaluator(a, b, c).mean)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h1RQGOBjn4w_"
      },
      "source": [
        "Try to change the value of bn to see what performance you can get. In pratice, we will leverage an automated system to search over a set of possible transfromations to find an optimal one."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "include_colab_link": true,
      "name": "2-tensor-program-abstraction.ipynb",
      "provenance": []
    },
    "interpreter": {
      "hash": "ba5575bccd128f2b7882a651373088b5bcda834434b08a7690be17ec2b08fe59"
    },
    "kernelspec": {
      "display_name": "Python 3.7.12 ('lxyMLC')",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
